    [core]
    dags_folder = /usr/local/airflow/dags
    base_log_folder = /usr/local/airflow/logs

    remote_logging = False
    remote_log_conn_id =
    remote_base_log_folder =
    encrypt_s3_logs = False

    # The executor class that airflow should use. Choices include
    # SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor
    executor = LocalExecutor

    # sql_alchemy_conn = AIRFLOW__CORE__SQL_ALCHEMY_CONN from manifest
    sql_alchemy_conn =
    load_examples = False
    # fernet_key = AIRFLOW__CORE__FERNET_KEY
    fernet_key =

    [cli]
    api_client = airflow.api.client.local_client
    endpoint_url = http://my-airflow.example.cool

    [api]
    auth_backend = airflow.api.auth.backend.default

    [webserver]
    base_url = http://my-airflow.example.cool
    web_server_host = 0.0.0.0
    web_server_port = 8080

    # Set to true to turn on authentication:
    # https://airflow.apache.org/security.html#web-authentication
    authenticate = True
    auth_backend = airflow.contrib.auth.backends.password_auth
    # Use FAB-based webserver with RBAC feature
    rbac = True
    expose_config = True

    [scheduler]
    job_heartbeat_sec = 5
    scheduler_heartbeat_sec = 5
    run_duration = -1
    min_file_process_interval = 5
    dag_dir_list_interval = 300
    print_stats_interval = 30
    scheduler_health_check_threshold = 30
    child_process_log_directory = /usr/local/airflow/logs/scheduler
    scheduler_zombie_task_threshold = 300
    catchup_by_default = True
    max_tis_per_query = 512
    max_threads = 2
    authenticate = False
    use_job_schedule = True

    [admin]
    hide_sensitive_variable_fields = True

    # TODO: setup elasticsearch for logging
    #[elasticsearch]
    ## Elasticsearch host
    #host =
    ## Format of the log_id, which is used to query for a given tasks logs
    #log_id_template = {{dag_id}}-{{task_id}}-{{execution_date}}-{{try_number}}
    ## Used to mark the end of a log stream for a task
    #end_of_log_mark = end_of_log
    ## Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
    ## Code will construct log_id using the log_id template from the argument above.
    ## NOTE: The code will prefix the https:// automatically, don't include that here.
    #frontend =
    ## Write the task logs to the stdout of the worker, rather than the default files
    #write_stdout = False
    ## Instead of the default log formatter, write the log lines as JSON
    #json_format = False
    ## Log fields to also attach to the json output, if enabled
    #json_fields = asctime, filename, lineno, levelname, message
    #[elasticsearch_configs]
    #use_ssl = True
    #verify_certs = True
    #ca_certs = /es/certs/ca.pem
